{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd3009a",
   "metadata": {},
   "source": [
    "# Data Science Final Project\n",
    "\n",
    "\n",
    "For my final project, I chose to utilize Natural Language Processing (NLP) for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a0f2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tdepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# install packages (for use on a differect computer)\n",
    "# !pip install --user --upgrade nltk\n",
    "# !pip install --user --upgrade keras\n",
    "# !pip install --user --upgrade tensorflow\n",
    "# !pip install --user --upgrade textblob\n",
    "# !pip install --user --upgrade tqdm\n",
    "# !pip install --user --upgrade time\n",
    "\n",
    "# for progress bars\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from dateutil import parser\n",
    "import string\n",
    "import keras\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.max_colwidth = 1000\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e15677",
   "metadata": {},
   "source": [
    "Here are some of the references that I used (for downloading the data and with code help)\n",
    "\n",
    "https://www.kaggle.com/datasets/kazanova/sentiment140\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/06/twitter-sentiment-analysis-a-nlp-use-case-for-beginners/\n",
    "\n",
    "https://www.kaggle.com/datasets/columbine/imdb-dataset-sentiment-analysis-in-csv-format\n",
    "\n",
    "https://www.kaggle.com/datasets/gpreda/covid19-tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4a9706",
   "metadata": {},
   "source": [
    "## Importing Dataset\n",
    "\n",
    "Here, I import the dataset containing the pre-analyzed tweets. I fix the dataframe a little to work better with the machine learning model. I also shuffle the dataset, as it is originally sorted with all positives first and then negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97df18a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment      int32\n",
       "id           float64\n",
       "date          object\n",
       "query         object\n",
       "user          object\n",
       "tweet         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('twitterdata.csv', encoding = \"ISO-8859-1\")\n",
    "print('Loaded file.')\n",
    "df = pd.DataFrame(np.vstack([df.columns, df])) # Moves column names into row 1\n",
    "df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'tweet'] # Renames columns\n",
    "df.replace({'sentiment': {4: 1}}, inplace=True) # Replaces all '4's with '1's in column 'sentiment'\n",
    "df.replace({'sentiment': {0: -1}}, inplace=True) # Replaces all '0's with '-1's in column 'sentiment'\n",
    "data_types_dict = {\n",
    "    'sentiment': int,\n",
    "    'id': float\n",
    "}\n",
    "df = df.astype(data_types_dict) # changes value types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b1cf51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>Just in newry...me and cat are going to buy a tent! Summer 2008 people! Its raining now though.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@zey_rochelle The one we love/like may not always be the right one for us.Time heals wonds,pain,and sorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Finally got rid of the Smart Board tools automatic launch on login on my Mac Leopard.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>Penultimate Pushing Daisies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>@jeramyer haha nope aint gonna work as we hav a bday list up so they no it aint my bday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  \\\n",
       "0         -1   \n",
       "1          1   \n",
       "2          1   \n",
       "3         -1   \n",
       "4         -1   \n",
       "\n",
       "                                                                                                         tweet  \n",
       "0             Just in newry...me and cat are going to buy a tent! Summer 2008 people! Its raining now though.   \n",
       "1  @zey_rochelle The one we love/like may not always be the right one for us.Time heals wonds,pain,and sorrow   \n",
       "2                       Finally got rid of the Smart Board tools automatic launch on login on my Mac Leopard.   \n",
       "3                                                                                 Penultimate Pushing Daisies   \n",
       "4                     @jeramyer haha nope aint gonna work as we hav a bday list up so they no it aint my bday   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True) # shuffles rows so its not all 1 and then -1\n",
    "df = df[['sentiment', 'tweet']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "397db2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                                                                                                                                800000\n",
      "-1                                                                                                                               799999\n",
      "isPlayer Has Died! Sorry                                                                                                            210\n",
      "good morning                                                                                                                        118\n",
      "headache                                                                                                                            115\n",
      "                                                                                                                                  ...  \n",
      "mmm 2 hrs left at work then bed think me is coming down with a bug                                                                    1\n",
      "@SelanneGirl Saku Koivu in Anaheim    YES!!!   Oh what the heck, Let's keep it going.                                                 1\n",
      "man keith urban didnt perform my fav song                                                                                             1\n",
      "@archrlatina my son named him Shadow                                                                                                  1\n",
      "everything about my dad annoys me. i can honestly say i dont love him i dont like him im only here because i have no choice.          1\n",
      "Length: 1581469, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "s = df.stack().value_counts()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d3a99",
   "metadata": {},
   "source": [
    "Here, since the dataset is very large, I can limit the amount of rows used to run faster while testing as `cutoff`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80328059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Just in newry...me and cat are going to buy a tent! Summer 2008 people! Its raining now though.',\n",
       " '@zey_rochelle The one we love/like may not always be the right one for us.Time heals wonds,pain,and sorrow',\n",
       " 'Finally got rid of the Smart Board tools automatic launch on login on my Mac Leopard.',\n",
       " 'Penultimate Pushing Daisies',\n",
       " '@jeramyer haha nope aint gonna work as we hav a bday list up so they no it aint my bday',\n",
       " 'Missing out on Manhattanhenge',\n",
       " \"@ddlovato OMG DEMI YOU ARE AWESOME! YOU ROCK!I CAN'T WAIT FOR YOUR NEW CD.. THE COVER SEEMS SO COOL  I LOVE U SO MUCH!\",\n",
       " '@PawPrintsMag it appears the shorter ones and being used already   Any other suggestions for possible 2 or 3 letter tags?',\n",
       " 'Come check out JavaOne DAY 3 on twazzup! http://javaone.twazzup.com If you want your own event page, DM @twazzup',\n",
       " 'Not a cloud in the sky!! So beautiful out!!! To bad its wasted on work!',\n",
       " 'Updating my bookclub site. http://bookclub.meetup.com/1316 Replying to emails. About to list a poll for our July and August books',\n",
       " '@_carnavas its small and behind my ear, easily covered',\n",
       " \"I woke up at 5 am , I had nightmare, I'm shacking and my chest in the side of the heart is hurt\",\n",
       " 'Fixing to take a brutal test, I have had zero fun the last 3 days...study and work all the time',\n",
       " \"Working again. Off tomorrow. Text it. Btw people who ignore my texts are losers. I'll remember it\",\n",
       " '@RobbySTEREOS   planning to take over the world... lol jk... well to go to S.C.E.N.E. Fest ... wootwoot!!',\n",
       " '@NathanMillson 10mins until your roadtrip!!! Excited?',\n",
       " 'tonight was fun',\n",
       " '@Aprilknob I see you...',\n",
       " '@ThankTank']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutoff = 5000\n",
    "content = df['tweet'][:cutoff].values.tolist()\n",
    "labels = df['sentiment'][:cutoff].values.tolist()\n",
    "content = [x.strip() for x in content] # Deletes white space before and after\n",
    "content[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e261e",
   "metadata": {},
   "source": [
    "We create `y` as a numpy array of all labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4308341a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1, -1, -1, -1,  1, -1,  1, -1,  1,  1, -1, -1,  1,  1,  1,\n",
       "        1,  1,  1], dtype=int8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(labels, dtype='int8')\n",
    "y[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae2b56f",
   "metadata": {},
   "source": [
    "## Stop Words #1\n",
    "\n",
    "We create a dataframe `content1` that is processed through nltk's list of stop words. Basically, we go through each tweet and remove each of the 'stop words' that do not add any meaning to make it easier for the model to train on. It also removes other unneccesary information, such as numbers and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8c0a82",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stops' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     newtxt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m txt\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopWords])\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m newtxt\n\u001b[1;32m---> 22\u001b[0m content1 \u001b[38;5;241m=\u001b[39m [removeStopWords(stops,x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m sents_lower]\n\u001b[0;32m     23\u001b[0m content1[:\u001b[38;5;241m20\u001b[39m]\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     20\u001b[0m     newtxt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m txt\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopWords])\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m newtxt\n\u001b[1;32m---> 22\u001b[0m content1 \u001b[38;5;241m=\u001b[39m [removeStopWords(\u001b[43mstops\u001b[49m,x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m sents_lower]\n\u001b[0;32m     23\u001b[0m content1[:\u001b[38;5;241m20\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stops' is not defined"
     ]
    }
   ],
   "source": [
    "def full_remove(x, removal_list):\n",
    "    # function for removing the stop words\n",
    "    for w in removal_list:\n",
    "        x = x.replace(w, ' ')\n",
    "    return x\n",
    "\n",
    "# remove all digits\n",
    "digits = [str(x) for x in range(10)]\n",
    "remove_digits = [full_remove(x, digits) for x in content]\n",
    "\n",
    "# remove all punctuation\n",
    "remove_punc = [full_remove(x, list(string.punctuation)) for x in remove_digits]\n",
    "\n",
    "# make everything lower-case and remove any white space\n",
    "sents_lower = [x.lower() for x in remove_punc]\n",
    "sents_lower = [x.strip() for x in sents_lower]\n",
    "\n",
    "# remove stop words\n",
    "def removeStopWords(stopWords, txt):\n",
    "    newtxt = ' '.join([word for word in txt.split() if word not in stopWords])\n",
    "    return newtxt\n",
    "content1 = [removeStopWords(stops,x) for x in sents_lower]\n",
    "content1[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78463000",
   "metadata": {},
   "source": [
    "## Vectorizing\n",
    "\n",
    "explain here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e42eae6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(analyzer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      2\u001b[0m                              preprocessor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[0;32m      3\u001b[0m                              stop_words \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      4\u001b[0m                              max_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6000\u001b[39m, ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m data_features \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mcontent2\u001b[49m)\n\u001b[0;32m      6\u001b[0m tfidf_transformer \u001b[38;5;241m=\u001b[39m TfidfTransformer()\n\u001b[0;32m      7\u001b[0m data_features_tfidf \u001b[38;5;241m=\u001b[39m tfidf_transformer\u001b[38;5;241m.\u001b[39mfit_transform(data_features)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'content2' is not defined"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                             preprocessor = None, \n",
    "                             stop_words =  'english', \n",
    "                             max_features = 6000, ngram_range=(1,5))\n",
    "data_features = vectorizer.fit_transform(content2)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "data_features_tfidf = tfidf_transformer.fit_transform(data_features)\n",
    "data_mat = data_features_tfidf.toarray()\n",
    "\n",
    "np.random.seed(0)\n",
    "test_index = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "train_index = list(set(range(len(labels))) - set(test_index))\n",
    "train_data = data_mat[train_index,]\n",
    "train_labels = y[train_index]\n",
    "test_data = data_mat[test_index,]\n",
    "test_labels = y[test_index]\n",
    "\n",
    "# create polarity function and subjectivity function\n",
    "pol = lambda x: TextBlob(x).sentiment.polarity\n",
    "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "pol_list = [pol(x) for x in content2]\n",
    "sub_list = [sub(x) for x in content2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cfd8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(content2[i], '\\t', pol_list[i], sub_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit logistic classifier on training data\n",
    "clf = SGDClassifier(loss=\"log\", penalty=\"none\")\n",
    "clf.fit(train_data, train_labels)\n",
    "## Pull out the parameters (w,b) of the logistic regression model\n",
    "w = clf.coef_[0,:]\n",
    "b = clf.intercept_\n",
    "## Get predictions on training and test data\n",
    "preds_train = clf.predict(train_data)\n",
    "preds_test = clf.predict(test_data)\n",
    "## Compute errors\n",
    "errs_train = np.sum((preds_train > 0.0) != (train_labels > 0.0))\n",
    "errs_test = np.sum((preds_test > 0.0) != (test_labels > 0.0))\n",
    "print(\"Training error: \", float(errs_train)/len(train_labels))\n",
    "print(\"Test error: \", float(errs_test)/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab07ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert vocabulary into a list:\n",
    "vocab = np.array([z[0] for z in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])])\n",
    "## Get indices of sorting w\n",
    "inds = np.argsort(w)\n",
    "## Words with large negative values\n",
    "neg_inds = inds[0:50]\n",
    "print(\"Highly negative words: \")\n",
    "# MB: fixed bug here\n",
    "print([x for x in list(vocab[neg_inds])])\n",
    "## Words with large positive values\n",
    "pos_inds = inds[-49:-1]\n",
    "print(\"Highly positive words: \")\n",
    "print([x for x in list(vocab[pos_inds])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badb01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.predict(vectorizer.transform([\"It's a sad movie but very good\"])))\n",
    "print(clf.predict(vectorizer.transform([\"Waste of my time\"])))\n",
    "print(clf.predict(vectorizer.transform([\"It is not what like\"])))\n",
    "print(clf.predict(vectorizer.transform([\"It is not what I m looking for\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09871006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "svm_clf = SGDClassifier(loss=\"hinge\", penalty='l2')\n",
    "svm_clf.fit(train_data, train_labels)\n",
    "svm_preds_test = svm_clf.predict(test_data)\n",
    "svm_errs_test = np.sum((svm_preds_test > 0.0) != (test_labels > 0.0))\n",
    "print(\"Test error: \", float(svm_errs_test)/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54900f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svm_clf.predict(vectorizer.transform([\"It's a sad movie but very good\"])))\n",
    "print(svm_clf.predict(vectorizer.transform([\"Waste of my time\"])))\n",
    "print(svm_clf.predict(vectorizer.transform([\"This is not what I like\"])))\n",
    "print(svm_clf.predict(vectorizer.transform([\"It is not what I am looking for\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538d272",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = 200\n",
    "tokenizer = Tokenizer(num_words=10000,  #max no. of unique words to keep\n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', \n",
    "                      lower=True #convert to lower case\n",
    "                     )\n",
    "tokenizer.fit_on_texts(content2)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(content2)\n",
    "X = pad_sequences(X, maxlen= max_review_length)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "\n",
    "Y=pd.get_dummies(y).values\n",
    "\n",
    "np.random.seed(0)\n",
    "test_inds = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "train_inds = list(set(range(len(labels))) - set(test_inds))\n",
    "train_data = X[train_inds,]\n",
    "train_labels = Y[train_inds]\n",
    "test_data = X[test_inds,]\n",
    "test_labels = Y[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586a9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(250, dropout=0.2,return_sequences=True))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a3d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 40\n",
    "model.fit(train_data, train_labels, \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff243fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_data, test_labels, verbose=2,\n",
    "                            batch_size=batch_size)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Validation accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_labels = ['Negative', 'Positive']\n",
    "new = [\"test\"]\n",
    " \n",
    "def predict_sentiment(text, pr):\n",
    "    seq = tokenizer.texts_to_sequences(text)\n",
    "    padded = pad_sequences(seq, maxlen=max_review_length)\n",
    "    if pr:\n",
    "        pred = model.predict(padded)\n",
    "        print(\"Probability distribution: \", pred)\n",
    "        print(f\"Is this a Positive or Negative message? '{text[0]}'\")\n",
    "        print(outcome_labels[np.argmax(pred)])\n",
    "    else:\n",
    "        pred = model.predict(padded, verbose=0)\n",
    "    return outcome_labels[np.argmax(pred)]\n",
    "\n",
    "predict_sentiment(new, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f0142",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_sentiment(['kys'], True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82efb81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcovid = pd.read_csv('covid19_tweets.csv')\n",
    "dfcovid = dfcovid[['text']]\n",
    "dfcovid['text'] = dfcovid['text'].str.replace(r'https?://\\S+', '', case=False)\n",
    "dfcovid = dfcovid[:5000]\n",
    "dfcovid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952a32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcovid['result'] = dfcovid.progress_apply(lambda row: predict_sentiment([row.text], False), axis=1)\n",
    "dfcovid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d34b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seaborn to create a bar graph\n",
    "sns.countplot(x='result', data=dfcovid, palette='Blues')\n",
    "\n",
    "# Add labels and show the plot\n",
    "plt.xlabel('Result')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of -1 vs 1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77e7c14",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
