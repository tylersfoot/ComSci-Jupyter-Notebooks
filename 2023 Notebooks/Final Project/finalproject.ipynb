{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd3009a",
   "metadata": {},
   "source": [
    "# Data Science Final Project\n",
    "\n",
    "\n",
    "For my final project, I chose to utilize Natural Language Processing (NLP) for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29a0f2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tdepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# install packages (for use on a differect computer)\n",
    "# !pip install --user --upgrade nltk\n",
    "# !pip install --user --upgrade keras\n",
    "# !pip install --user --upgrade tensorflow\n",
    "# !pip install --user --upgrade textblob\n",
    "# !pip install --user --upgrade tqdm\n",
    "# !pip install --user --upgrade time\n",
    "\n",
    "# for progress bars\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from dateutil import parser\n",
    "import string\n",
    "import keras\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.max_colwidth = 1000\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e15677",
   "metadata": {},
   "source": [
    "Here are some of the references that I used (for downloading the data and with code help)\n",
    "\n",
    "https://www.kaggle.com/datasets/kazanova/sentiment140\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/06/twitter-sentiment-analysis-a-nlp-use-case-for-beginners/\n",
    "\n",
    "https://www.kaggle.com/datasets/columbine/imdb-dataset-sentiment-analysis-in-csv-format\n",
    "\n",
    "https://www.kaggle.com/datasets/gpreda/covid19-tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41c1d8d",
   "metadata": {},
   "source": [
    "## Importing Dataset\n",
    "\n",
    "Here, I import the dataset containing the pre-analyzed tweets. I fix the dataframe a little to work better with the machine learning model. I also shuffle the dataset, as it is originally sorted with all positives first and then negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97df18a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file.\n",
      "Fixed sentiment values.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment      int32\n",
       "id           float64\n",
       "date          object\n",
       "query         object\n",
       "user          object\n",
       "tweet         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('twitterdata.csv', encoding = \"ISO-8859-1\")\n",
    "print('Loaded file.')\n",
    "df = pd.DataFrame(np.vstack([df.columns, df])) # Moves column names into row 1\n",
    "df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'tweet'] # Renames columns\n",
    "df.replace({'sentiment': {4: 1}}, inplace=True) # Replaces all '4's with '1's in column 'sentiment'\n",
    "df.replace({'sentiment': {0: -1}}, inplace=True) # Replaces all '0's with '-1's in column 'sentiment'\n",
    "data_types_dict = {\n",
    "    'sentiment': int,\n",
    "    'id': float\n",
    "}\n",
    "df = df.astype(data_types_dict) # changes value types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b1cf51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>@ddalisay621, yep we have the same Anni. It's creepy  I miss ya! We needa chill..very soon! Haha. Im going shopping with my cousin on wed!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@jerrodr Lazy days are good..especially with what's headed our way this week.  Glad you had a good weekend.  Mine was good, too!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>Just got home from work and it's my turn to do dishes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>hey guys!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>Received the #brotherscider festival pack today. Shame I'm not going to glasto so no free hooch for me  Least I got a free hat!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  \\\n",
       "0         -1   \n",
       "1          1   \n",
       "2         -1   \n",
       "3          1   \n",
       "4         -1   \n",
       "\n",
       "                                                                                                                                        tweet  \n",
       "0  @ddalisay621, yep we have the same Anni. It's creepy  I miss ya! We needa chill..very soon! Haha. Im going shopping with my cousin on wed!  \n",
       "1           @jerrodr Lazy days are good..especially with what's headed our way this week.  Glad you had a good weekend.  Mine was good, too!   \n",
       "2                                                                                     Just got home from work and it's my turn to do dishes.   \n",
       "3                                                                                                                                  hey guys!   \n",
       "4             Received the #brotherscider festival pack today. Shame I'm not going to glasto so no free hooch for me  Least I got a free hat!  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True) # shuffles rows so its not all 1 and then -1\n",
    "df = df[['sentiment', 'tweet']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "397db2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                                                                                                                                               800000\n",
      "-1                                                                                                                                              799999\n",
      "isPlayer Has Died! Sorry                                                                                                                           210\n",
      "good morning                                                                                                                                       118\n",
      "headache                                                                                                                                           115\n",
      "                                                                                                                                                 ...  \n",
      "@aussiecynic I may have to peek eh. Especially if I have some restoration to do eh                                                                   1\n",
      "goood night. gOS. from james brown to james green                                                                                                    1\n",
      "@jeffpulver As a Biz Mentor &amp; Entrepreneur based in dynamic Asia - I am emphatically having the business time of my life  Ciao Tweeters!         1\n",
      "hiya. heather is hacking into my computer not good                                                                                                   1\n",
      "@harveys wii sports sets you free my man                                                                                                             1\n",
      "Length: 1581469, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "s = df.stack().value_counts()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80328059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"@ddalisay621, yep we have the same Anni. It's creepy  I miss ya! We needa chill..very soon! Haha. Im going shopping with my cousin on wed!\",\n",
       " \"@jerrodr Lazy days are good..especially with what's headed our way this week.  Glad you had a good weekend.  Mine was good, too!\",\n",
       " \"Just got home from work and it's my turn to do dishes.\",\n",
       " 'hey guys!',\n",
       " \"Received the #brotherscider festival pack today. Shame I'm not going to glasto so no free hooch for me  Least I got a free hat!\",\n",
       " '@tommcfly reply to our random tweets, then fly over lovely England, a few more tweets and then rest for tomorrows opening night  good ;)',\n",
       " \"Don't worry - we're from the internets. It's going to be alright.\",\n",
       " \"watching VG Topp 20-lista, I can't be there\",\n",
       " '@MaggieBrown5 dont you know where you are from?? Mayfield all the way! (unless its my dads homemade milky way!)',\n",
       " 'so, this pretty much sucks, the guy i like i only get to see at school, and summers almost here. so, i pretty much have to give up, sh!t.',\n",
       " \"can't believe vaca is over  Working tomorrow 1-8:30!\",\n",
       " '@__cinnamon__ hey! haahaha school tomorrow thank god its friday i missed miss wetherall do the cat today  oh im sad i missed it haha',\n",
       " '@Kate_Kennedy unfortunately not  I had to reroute my trip to get home  earlier to see TAI, Carolina Liar, &amp; Empires, 3 days in a row',\n",
       " 'making dinner and getting ready for the Candis Cayne show at the Abbey with the boys!',\n",
       " \"@andrea3k will there be a download? Away from pooter. Can't tune in\",\n",
       " '@AshleyLTMSYF wait? did you get married? im so confused',\n",
       " 'OWWWWWWW FLO RIDA is coming HELSINKI in July  be there!! gonna be HOT!!! apple bottom jeans......',\n",
       " '@KimKardashian i was watchin tv yesterday and the media is on u. u were number five i think for worst beach bodies because of ur butt',\n",
       " \"I'm stumped hahah I wanted to go late night\",\n",
       " 'My eyes hurt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(10):\n",
    "#     print(df['tweet'][i])\n",
    "content = df['tweet'][:5000].values.tolist()\n",
    "labels = df['sentiment'][:5000].values.tolist()\n",
    "content = [x.strip() for x in content] # Deletes white space before and after\n",
    "content[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4308341a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1, -1,  1, -1,  1,  1, -1,  1, -1, -1, -1, -1,  1, -1, -1,  1,\n",
       "       -1, -1, -1], dtype=int8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(labels, dtype='int8')\n",
    "y[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d0b5e3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# REWORKED WITH REVIEWS DATASET\n",
    "# df = dfreviews\n",
    "# content = df['text'].values.tolist()\n",
    "# labels = df['label'].values.tolist()\n",
    "# content = [x.strip() for x in content] # Deletes white space before and after\n",
    "# print(content[:20])\n",
    "# y = np.array(labels, dtype='int8')\n",
    "# print(y[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae2b56f",
   "metadata": {},
   "source": [
    "## stop words #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d8c0a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ddalisay yep anni creepy miss ya needa chill soon haha im going shopping cousin wed',\n",
       " 'jerrodr lazy days good especially headed way week glad good weekend mine good',\n",
       " 'got home work turn dishes',\n",
       " 'hey guys',\n",
       " 'received brotherscider festival pack today shame going glasto free hooch least got free hat',\n",
       " 'tommcfly reply random tweets fly lovely england tweets rest tomorrows opening night good',\n",
       " 'worry internets going alright',\n",
       " 'watching vg topp lista',\n",
       " 'maggiebrown dont know mayfield way unless dads homemade milky way',\n",
       " 'pretty much sucks guy like get see school summers almost pretty much give sh',\n",
       " 'believe vaca working tomorrow',\n",
       " 'cinnamon hey haahaha school tomorrow thank god friday missed miss wetherall cat today oh im sad missed haha',\n",
       " 'kate kennedy unfortunately reroute trip get home earlier see tai carolina liar amp empires days row',\n",
       " 'making dinner getting ready candis cayne show abbey boys',\n",
       " 'andrea k download away pooter tune',\n",
       " 'ashleyltmsyf wait get married im confused',\n",
       " 'owwwwwww flo rida coming helsinki july gonna hot apple bottom jeans',\n",
       " 'kimkardashian watchin tv yesterday media u u number five think worst beach bodies ur butt',\n",
       " 'stumped hahah wanted go late night',\n",
       " 'eyes hurt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def full_remove(x, removal_list):\n",
    "    for w in removal_list:\n",
    "        x = x.replace(w, ' ')\n",
    "    return x\n",
    "\n",
    "## Remove digits\n",
    "digits = [str(x) for x in range(10)]\n",
    "remove_digits = [full_remove(x, digits) for x in content]\n",
    "\n",
    "## Remove punctuation\n",
    "remove_punc = [full_remove(x, list(string.punctuation)) for x in remove_digits]\n",
    "\n",
    "## Make everything lower-case and remove any white space\n",
    "sents_lower = [x.lower() for x in remove_punc]\n",
    "sents_lower = [x.strip() for x in sents_lower]\n",
    "\n",
    "## Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words(\"English\")\n",
    "def removeStopWords(stopWords, txt):\n",
    "    newtxt = ' '.join([word for word in txt.split() if word not in stopWords])\n",
    "    return newtxt\n",
    "content1 = [removeStopWords(stops,x) for x in sents_lower]\n",
    "content1[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76465a0",
   "metadata": {},
   "source": [
    "## stops we defined instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de5a9887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ddalisay yep we have same anni s creepy miss ya we needa chill very soon haha im going shopping with my cousin on wed',\n",
       " 'jerrodr lazy days are good especially with what s headed our way this week glad you had good weekend mine was good too',\n",
       " 'just got home work and s my turn do dishes',\n",
       " 'hey guys',\n",
       " 'received brotherscider festival pack today shame m not going glasto so no free hooch for me least got free hat',\n",
       " 'tommcfly reply our random tweets then fly over lovely england few more tweets and then rest for tomorrows opening night good',\n",
       " 'don t worry we re internets s going be alright',\n",
       " 'watching vg topp lista can t be there',\n",
       " 'maggiebrown dont you know where you are mayfield all way unless its my dads homemade milky way',\n",
       " 'so this pretty much sucks guy like only get see at school and summers almost here so pretty much have give up sh t',\n",
       " 'can t believe vaca is over working tomorrow',\n",
       " 'cinnamon hey haahaha school tomorrow thank god its friday missed miss wetherall do cat today oh im sad missed haha',\n",
       " 'kate kennedy unfortunately not had reroute my trip get home earlier see tai carolina liar amp empires days in row',\n",
       " 'making dinner and getting ready for candis cayne show at abbey with boys',\n",
       " 'andrea k will there be download away pooter can t tune in',\n",
       " 'ashleyltmsyf wait did you get married im so confused',\n",
       " 'owwwwwww flo rida is coming helsinki in july be there gonna be hot apple bottom jeans',\n",
       " 'kimkardashian was watchin tv yesterday and media is on u u were number five think for worst beach bodies because ur butt',\n",
       " 'm stumped hahah wanted go late night',\n",
       " 'my eyes hurt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_set = ['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from']\n",
    "content2 = [removeStopWords(stop_set,x) for x in sents_lower]\n",
    "content2[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281d13fa",
   "metadata": {},
   "source": [
    "## stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e417cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# def stemporter(words):\n",
    "#     porter = nltk.PorterStemmer()\n",
    "#     new_words = [porter.stem(w) for w in words]\n",
    "#     return new_words\n",
    "    \n",
    "# def stemlancaster(words):\n",
    "#     porter = nltk.LancasterStemmer()\n",
    "#     new_words = [porter.stem(w) for w in words]\n",
    "#     return new_words    \n",
    "\n",
    "# porter = [stemporter(x.split()) for x in content2]\n",
    "# porter = [\" \".join(i) for i in porter]\n",
    "# porter[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78463000",
   "metadata": {},
   "source": [
    "## vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e42eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                             preprocessor = None, \n",
    "                             stop_words =  'english', \n",
    "                             max_features = 6000, ngram_range=(1,5))\n",
    "data_features = vectorizer.fit_transform(content2)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "data_features_tfidf = tfidf_transformer.fit_transform(data_features)\n",
    "data_mat = data_features_tfidf.toarray()\n",
    "\n",
    "np.random.seed(0)\n",
    "test_index = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "train_index = list(set(range(len(labels))) - set(test_index))\n",
    "train_data = data_mat[train_index,]\n",
    "train_labels = y[train_index]\n",
    "test_data = data_mat[test_index,]\n",
    "test_labels = y[test_index]\n",
    "\n",
    "#Create polarity function and subjectivity function\n",
    "pol = lambda x: TextBlob(x).sentiment.polarity\n",
    "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "pol_list = [pol(x) for x in content2]\n",
    "sub_list = [sub(x) for x in content2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3cfd8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ddalisay yep we have same anni s creepy miss ya we needa chill very soon haha im going shopping with my cousin on wed \t -0.024999999999999994 0.43125\n",
      "jerrodr lazy days are good especially with what s headed our way this week glad you had good weekend mine was good too \t 0.3916666666666666 0.8000000000000002\n",
      "just got home work and s my turn do dishes \t 0.0 0.0\n",
      "hey guys \t 0.0 0.0\n",
      "received brotherscider festival pack today shame m not going glasto so no free hooch for me least got free hat \t -0.033333333333333326 0.6666666666666666\n",
      "tommcfly reply our random tweets then fly over lovely england few more tweets and then rest for tomorrows opening night good \t 0.3 0.5583333333333333\n",
      "don t worry we re internets s going be alright \t 0.0 0.0\n",
      "watching vg topp lista can t be there \t 0.0 0.0\n",
      "maggiebrown dont you know where you are mayfield all way unless its my dads homemade milky way \t 0.0 0.0\n",
      "so this pretty much sucks guy like only get see at school and summers almost here so pretty much have give up sh t \t 0.08 0.7\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(content2[i], '\\t', pol_list[i], sub_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb1a457f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error:  0.041777777777777775\n",
      "Test error:  0.32\n"
     ]
    }
   ],
   "source": [
    "## Fit logistic classifier on training data\n",
    "clf = SGDClassifier(loss=\"log\", penalty=\"none\")\n",
    "clf.fit(train_data, train_labels)\n",
    "## Pull out the parameters (w,b) of the logistic regression model\n",
    "w = clf.coef_[0,:]\n",
    "b = clf.intercept_\n",
    "## Get predictions on training and test data\n",
    "preds_train = clf.predict(train_data)\n",
    "preds_test = clf.predict(test_data)\n",
    "## Compute errors\n",
    "errs_train = np.sum((preds_train > 0.0) != (train_labels > 0.0))\n",
    "errs_test = np.sum((preds_test > 0.0) != (test_labels > 0.0))\n",
    "print(\"Training error: \", float(errs_train)/len(train_labels))\n",
    "print(\"Test error: \", float(errs_test)/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dab07ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly negative words: \n",
      "['sad', 'miss', 'hate', 'sucks', 'headache', 'stupid', 'sadly', 'dunno', 'boo', 'hurt', 'crap', 'disappointed', 'missing', 'exam', 'sorry', 'upset', 'isn', 'agh', 'queues', 'tried', 'record', 'ugh', 'miss dad', 'miserable', 'hrs', 'tv', 'didnt', 'confused', 'broke', 'raining', 'wtf', 'slower', 'late', 'crying', 'hates', 'wasnt', 'club', 'got excited', 'boring', 'lately', 'parties', 'ive', 'calling', 'damn', 'board', 'bad', 'thursday', 'download', 'qwerty', 'started']\n",
      "Highly positive words: \n",
      "['present', 'salads', 'dinner', 'mark', 'surprise', 'happy', 'banksyart', 'hehe', 'just woke', 'good morning', 'agreed', 'spammers', 'awesome', 'smile', 'wait th', 'pity', 'em', 'graduating', 'hanging', 'bright', 'great', 'consider', 'day today', 'love', 'pic', 'hr', 'lovely', 'assignments', 'loves', 'petemc', 'cool', 'cute', 'kaccat', 'ha', 'listening', 'fly', 'tht', 'amazing', 'magic', 'thanx', 'damned', 'mall', 'thank', 'thanks', 'loved', 'yea', 'ff', 'excited']\n"
     ]
    }
   ],
   "source": [
    "## Convert vocabulary into a list:\n",
    "vocab = np.array([z[0] for z in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])])\n",
    "## Get indices of sorting w\n",
    "inds = np.argsort(w)\n",
    "## Words with large negative values\n",
    "neg_inds = inds[0:50]\n",
    "print(\"Highly negative words: \")\n",
    "# MB: fixed bug here\n",
    "print([x for x in list(vocab[neg_inds])])\n",
    "## Words with large positive values\n",
    "pos_inds = inds[-49:-1]\n",
    "print(\"Highly positive words: \")\n",
    "print([x for x in list(vocab[pos_inds])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "badb01b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1]\n",
      "[1]\n",
      "[-1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(vectorizer.transform([\"It's a sad movie but very good\"])))\n",
    "print(clf.predict(vectorizer.transform([\"Waste of my time\"])))\n",
    "print(clf.predict(vectorizer.transform([\"It is not what like\"])))\n",
    "print(clf.predict(vectorizer.transform([\"It is not what I m looking for\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09871006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error:  0.346\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "svm_clf = SGDClassifier(loss=\"hinge\", penalty='l2')\n",
    "svm_clf.fit(train_data, train_labels)\n",
    "svm_preds_test = svm_clf.predict(test_data)\n",
    "svm_errs_test = np.sum((svm_preds_test > 0.0) != (test_labels > 0.0))\n",
    "print(\"Test error: \", float(svm_errs_test)/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54900f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1]\n",
      "[1]\n",
      "[-1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(svm_clf.predict(vectorizer.transform([\"It's a sad movie but very good\"])))\n",
    "print(svm_clf.predict(vectorizer.transform([\"Waste of my time\"])))\n",
    "print(svm_clf.predict(vectorizer.transform([\"This is not what I like\"])))\n",
    "print(svm_clf.predict(vectorizer.transform([\"It is not what I am looking for\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9538d272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 200\n",
    "tokenizer = Tokenizer(num_words=10000,  #max no. of unique words to keep\n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', \n",
    "                      lower=True #convert to lower case\n",
    "                     )\n",
    "tokenizer.fit_on_texts(content2)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(content2)\n",
    "X = pad_sequences(X, maxlen= max_review_length)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "\n",
    "Y=pd.get_dummies(y).values\n",
    "\n",
    "np.random.seed(0)\n",
    "test_inds = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "train_inds = list(set(range(len(labels))) - set(test_inds))\n",
    "train_data = X[train_inds,]\n",
    "train_labels = Y[train_inds]\n",
    "test_data = X[test_inds,]\n",
    "test_labels = Y[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "586a9fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 200)          2000000   \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 200, 200)         0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 200, 250)          451000    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               140400    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,591,602\n",
      "Trainable params: 2,591,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(250, dropout=0.2,return_sequences=True))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46a3d566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "102/102 [==============================] - 116s 1s/step - loss: 0.6462 - accuracy: 0.6217 - val_loss: 0.5641 - val_accuracy: 0.7244\n",
      "Epoch 2/2\n",
      "102/102 [==============================] - 115s 1s/step - loss: 0.4181 - accuracy: 0.8185 - val_loss: 0.5708 - val_accuracy: 0.7200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1903ed7ee80>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 2\n",
    "batch_size = 40\n",
    "model.fit(train_data, train_labels, \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff243fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 - 3s - loss: 0.5551 - accuracy: 0.7500 - 3s/epoch - 253ms/step\n",
      "Loss: 0.5551156401634216\n",
      "Validation accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_data, test_labels, verbose=2,\n",
    "                            batch_size=batch_size)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Validation accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb60558e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 915ms/step\n",
      "Probability distribution:  [[0.66658044 0.3334196 ]]\n",
      "Is this a Positive or Negative message? 'test'\n",
      "Negative\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome_labels = ['Negative', 'Positive']\n",
    "new = [\"test\"]\n",
    " \n",
    "def predict_sentiment(text, pr):\n",
    "    seq = tokenizer.texts_to_sequences(text)\n",
    "    padded = pad_sequences(seq, maxlen=max_review_length)\n",
    "    if pr:\n",
    "        pred = model.predict(padded)\n",
    "        print(\"Probability distribution: \", pred)\n",
    "        print(f\"Is this a Positive or Negative message? '{text[0]}'\")\n",
    "        print(outcome_labels[np.argmax(pred)])\n",
    "    else:\n",
    "        pred = model.predict(padded, verbose=0)\n",
    "    return outcome_labels[np.argmax(pred)]\n",
    "\n",
    "predict_sentiment(new, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e2f0142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 83ms/step\n",
      "Probability distribution:  [[0.45211548 0.5478846 ]]\n",
      "Is this a Positive or Negative message? 'kys'\n",
      "Positive\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(['kys'], True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82efb81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tdepa\\AppData\\Local\\Temp\\ipykernel_3256\\1420892159.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  dfcovid['text'] = dfcovid['text'].str.replace(r'https?://\\S+', '', case=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If I smelled the scent of hand sanitizers today on someone in the past, I would think they were so intoxicated thatâ€¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey @Yankees @YankeesPR and @MLB - wouldn't it have made more sense to have the players pay their respects to the Aâ€¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@diane3443 @wdunlap @realDonaldTrump Trump never once claimed #COVID19 was a hoax. We all claim that this effort toâ€¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@brookbanktv The one gift #COVID19 has give me is an appreciation for the simple things that were always around meâ€¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25 July : Media Bulletin on Novel #CoronaVirusUpdates #COVID19 \\n@kansalrohit69 @DrSyedSehrish @airnewsalerts @ANIâ€¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>U-23 coach David Prats had already taken Xaviâ€™s place at the pre-match press conference and there were rumours flyiâ€¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>That's your plan to stop the global  protest against corruption that started in 2019 ?\\n\\nU found friends  in China hâ€¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>If youâ€™re waiting for a break from bad news about COVID, donâ€™t hold your breath â€” or, actually,Â doÂ hold your breathâ€¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>#classicozo #RihannaXTiwa #alreadyvideo #bbnaijalockdown2020 #LucyOurMama #COVID19 \\nThis boy no go kill me ðŸ¤£ðŸ¤£ðŸ¤£</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>How to achieve a fit for purpose finance function during a crisis.\\n\\n#COVID19 | #Coronavirus | #Finance \\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                         text\n",
       "0       If I smelled the scent of hand sanitizers today on someone in the past, I would think they were so intoxicated thatâ€¦ \n",
       "1       Hey @Yankees @YankeesPR and @MLB - wouldn't it have made more sense to have the players pay their respects to the Aâ€¦ \n",
       "2       @diane3443 @wdunlap @realDonaldTrump Trump never once claimed #COVID19 was a hoax. We all claim that this effort toâ€¦ \n",
       "3        @brookbanktv The one gift #COVID19 has give me is an appreciation for the simple things that were always around meâ€¦ \n",
       "4        25 July : Media Bulletin on Novel #CoronaVirusUpdates #COVID19 \\n@kansalrohit69 @DrSyedSehrish @airnewsalerts @ANIâ€¦ \n",
       "...                                                                                                                       ...\n",
       "4995    U-23 coach David Prats had already taken Xaviâ€™s place at the pre-match press conference and there were rumours flyiâ€¦ \n",
       "4996  That's your plan to stop the global  protest against corruption that started in 2019 ?\\n\\nU found friends  in China hâ€¦ \n",
       "4997    If youâ€™re waiting for a break from bad news about COVID, donâ€™t hold your breath â€” or, actually,Â doÂ hold your breathâ€¦ \n",
       "4998         #classicozo #RihannaXTiwa #alreadyvideo #bbnaijalockdown2020 #LucyOurMama #COVID19 \\nThis boy no go kill me ðŸ¤£ðŸ¤£ðŸ¤£ \n",
       "4999            How to achieve a fit for purpose finance function during a crisis.\\n\\n#COVID19 | #Coronavirus | #Finance \\n\\n\n",
       "\n",
       "[5000 rows x 1 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfcovid = pd.read_csv('covid19_tweets.csv')\n",
    "dfcovid = dfcovid[['text']]\n",
    "dfcovid['text'] = dfcovid['text'].str.replace(r'https?://\\S+', '', case=False)\n",
    "dfcovid = dfcovid[:5000]\n",
    "dfcovid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952a32bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2466/5000 [07:06<09:00,  4.68it/s]"
     ]
    }
   ],
   "source": [
    "dfcovid['result'] = dfcovid.progress_apply(lambda row: predict_sentiment([row.text], False), axis=1)\n",
    "dfcovid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19412b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use seaborn to create a bar graph\n",
    "sns.countplot(x='result', data=dfcovid, palette='Blues')\n",
    "\n",
    "# Add labels and show the plot\n",
    "plt.xlabel('Result')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of -1 vs 1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f0ebb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
