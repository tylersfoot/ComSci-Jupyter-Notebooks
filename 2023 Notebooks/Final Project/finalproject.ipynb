{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd3009a",
   "metadata": {},
   "source": [
    "# Data Science Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a0f2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tdepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "# !pip install keras\n",
    "# !pip install tensorflow\n",
    "from matplotlib import pyplot\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from dateutil import parser\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e15677",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/kazanova/sentiment140\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/06/twitter-sentiment-analysis-a-nlp-use-case-for-beginners/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97df18a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file.\n",
      "Fixed sentiment values.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment      int32\n",
       "id           float64\n",
       "date          object\n",
       "query         object\n",
       "user          object\n",
       "tweet         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('twitterdata.csv', encoding = \"ISO-8859-1\")\n",
    "print('Loaded file.')\n",
    "df = pd.DataFrame(np.vstack([df.columns, df])) # Moves column names into row 1\n",
    "df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'tweet'] # Renames columns\n",
    "df.replace({'sentiment': {4: 1}}, inplace=True) # Replaces all '4's with '1's in column 'sentiment'\n",
    "df.replace({'sentiment': {0: -1}}, inplace=True) # Replaces all '0's with '-1's in column 'sentiment'\n",
    "data_types_dict = {\n",
    "    'sentiment': int,\n",
    "    'id': float\n",
    "}\n",
    "print('Fixed sentiment values.')\n",
    "# df['unix_time'] = pd.to_datetime(df['date'], format='%a %b %d %H:%M:%S %Z %Y', errors='coerce').astype(int) / 10**9\n",
    "# df.dropna(subset=['unix_time'], inplace=True)\n",
    "\n",
    "# df['unix_time'] = df['date'].apply(lambda x: parser.parse(x, tzinfos={\"PDT\": -7*3600}).timestamp())\n",
    "# print('Added unix column.')\n",
    "\n",
    "df = df.astype(data_types_dict)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b1cf51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>omg. i just spent the whole day reading and do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Twitter Wars: The New Hope  Now I just have to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>@DonnieWahlberg I have seen it, BOY you ROCK t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>Today is a make-up day I fear  no spots but pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>my accomplishment of rationality and reason: m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                              tweet\n",
       "0         -1  omg. i just spent the whole day reading and do...\n",
       "1          1  Twitter Wars: The New Hope  Now I just have to...\n",
       "2          1  @DonnieWahlberg I have seen it, BOY you ROCK t...\n",
       "3         -1  Today is a make-up day I fear  no spots but pa...\n",
       "4         -1  my accomplishment of rationality and reason: m..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv('twitterdata_unix.csv', encoding = \"ISO-8859-1\")\n",
    "# df = df[['sentiment', 'tweet', 'unix_time']]\n",
    "df = df.sample(frac=1).reset_index(drop=True) # shuffles rows so its not all 1 and then -1\n",
    "df = df[['sentiment', 'tweet']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "397db2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                                                                                                 800000\n",
      "-1                                                                                                799999\n",
      "isPlayer Has Died! Sorry                                                                             210\n",
      "good morning                                                                                         118\n",
      "headache                                                                                             115\n",
      "                                                                                                   ...  \n",
      "@codizzle009 wooow we are both awake  cool!                                                            1\n",
      "@mitsuba3 ã??ã?ã?ã?ã?²ã?¨ã?«ã?¯ã?@buzzter .  @p2pquak ã?£ã?¦ã?®ãã?ãã?ã?ã?ãï¼ï¼         1\n",
      "i'm sad that the cute boy was only here to try and sell me trackpants                                  1\n",
      "@mello826BA i never get my daily scope                                                                 1\n",
      "where is my phone &amp; good foods? aaaaaaaaah                                                         1\n",
      "Length: 1581469, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "s = df.stack().value_counts()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80328059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['omg. i just spent the whole day reading and doing this stupid packet.',\n",
       " 'Twitter Wars: The New Hope  Now I just have to figure out which mac app is the rebel/imperial guard.',\n",
       " '@DonnieWahlberg I have seen it, BOY you ROCK that LOGO out',\n",
       " 'Today is a make-up day I fear  no spots but patchy skin',\n",
       " \"my accomplishment of rationality and reason: my mom thinks it was the biggest mistake of my life and STILL won't drop it, how frustrating\",\n",
       " \"I'm not going to be a bitch, so I won't be choked by Celia after Dani writes a news article about Twitter\",\n",
       " '@Franklero haha ahhh frank you crazy kid',\n",
       " 'Night night all, I can see more Cluedo tomorrow  shall I just let them win so I can get some PEACE',\n",
       " '@ChaosMagick Really?! Oh, OK then, just for you   http://twitpic.com/7gk5o',\n",
       " 'Peeing while holding a squirmy baby is NOT an easy task!!! To make matters worse, Zoe was in the stall next 2 me pooping...    Ugh!!!',\n",
       " \"@phete I just tried in 0.5 experimental and framework 'webkit' works fine. I don't have 0.4 installed tho\",\n",
       " \"well it looks like it's gonna rain. no screen on the green.  dinner with the besties instead!\",\n",
       " \"@Rachecullen It sucks, doesn't it\",\n",
       " '@rah_rah i want a fine selection of biscuits',\n",
       " 'Company party tonight, should be fun! I hope somebody brings a beer bong',\n",
       " 'cuddles with my baby girl  and boyfriend! watched him play ball ! so fun! port is to hot!!! even at night you wanna be at the lake!',\n",
       " '@jojo611993 yay! i read that the pool wasnt done yet  poo! lol',\n",
       " 'not excited about working tomorrow at 6am!  had a GREAT weekend though!',\n",
       " 'Cleaning My Room, Unpacking &amp; Playing Drums',\n",
       " \"@alieninmusic pre-excitement - there's nothing like it  sounds almost like a catch phrase\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(10):\n",
    "#     print(df['tweet'][i])\n",
    "content = df['tweet'][:1000].values.tolist()\n",
    "content = [x.strip() for x in content] # Deletes white space before and after\n",
    "content[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae2b56f",
   "metadata": {},
   "source": [
    "## stop words #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8c0a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['omg spent whole day reading stupid packet',\n",
       " 'twitter wars new hope figure mac app rebel imperial guard',\n",
       " 'donniewahlberg seen boy rock logo',\n",
       " 'today make day fear spots patchy skin',\n",
       " 'accomplishment rationality reason mom thinks biggest mistake life still drop frustrating',\n",
       " 'going bitch choked celia dani writes news article twitter',\n",
       " 'franklero haha ahhh frank crazy kid',\n",
       " 'night night see cluedo tomorrow shall let win get peace',\n",
       " 'chaosmagick really oh ok http twitpic com gk',\n",
       " 'peeing holding squirmy baby easy task make matters worse zoe stall next pooping ugh',\n",
       " 'phete tried experimental framework webkit works fine installed tho',\n",
       " 'well looks like gonna rain screen green dinner besties instead',\n",
       " 'rachecullen sucks',\n",
       " 'rah rah want fine selection biscuits',\n",
       " 'company party tonight fun hope somebody brings beer bong',\n",
       " 'cuddles baby girl boyfriend watched play ball fun port hot even night wanna lake',\n",
       " 'jojo yay read pool wasnt done yet poo lol',\n",
       " 'excited working tomorrow great weekend though',\n",
       " 'cleaning room unpacking amp playing drums',\n",
       " 'alieninmusic pre excitement nothing like sounds almost like catch phrase']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def full_remove(x, removal_list):\n",
    "    for w in removal_list:\n",
    "        x = x.replace(w, ' ')\n",
    "    return x\n",
    "\n",
    "## Remove digits\n",
    "digits = [str(x) for x in range(10)]\n",
    "remove_digits = [full_remove(x, digits) for x in content]\n",
    "\n",
    "## Remove punctuation\n",
    "remove_punc = [full_remove(x, list(string.punctuation)) for x in remove_digits]\n",
    "\n",
    "## Make everything lower-case and remove any white space\n",
    "sents_lower = [x.lower() for x in remove_punc]\n",
    "sents_lower = [x.strip() for x in sents_lower]\n",
    "\n",
    "## Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words(\"English\")\n",
    "def removeStopWords(stopWords, txt):\n",
    "    newtxt = ' '.join([word for word in txt.split() if word not in stopWords])\n",
    "    return newtxt\n",
    "content1 = [removeStopWords(stops,x) for x in sents_lower]\n",
    "content1[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76465a0",
   "metadata": {},
   "source": [
    "## stops we defined instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de5a9887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['omg just spent whole day reading and doing this stupid packet',\n",
       " 'twitter wars new hope now just have figure out which mac app is rebel imperial guard',\n",
       " 'donniewahlberg have seen boy you rock that logo out',\n",
       " 'today is make up day fear no spots but patchy skin',\n",
       " 'my accomplishment rationality and reason my mom thinks was biggest mistake my life and still won t drop how frustrating',\n",
       " 'm not going be bitch so won t be choked by celia after dani writes news article about twitter',\n",
       " 'franklero haha ahhh frank you crazy kid',\n",
       " 'night night all can see more cluedo tomorrow shall just let them win so can get some peace',\n",
       " 'chaosmagick really oh ok then just for you http twitpic com gk o',\n",
       " 'peeing while holding squirmy baby is not easy task make matters worse zoe was in stall next me pooping ugh',\n",
       " 'phete just tried in experimental and framework webkit works fine don t have installed tho',\n",
       " 'well looks like s gonna rain no screen on green dinner with besties instead',\n",
       " 'rachecullen sucks doesn t',\n",
       " 'rah rah want fine selection biscuits',\n",
       " 'company party tonight should be fun hope somebody brings beer bong',\n",
       " 'cuddles with my baby girl and boyfriend watched him play ball so fun port is hot even at night you wanna be at lake',\n",
       " 'jojo yay read that pool wasnt done yet poo lol',\n",
       " 'not excited about working tomorrow at am had great weekend though',\n",
       " 'cleaning my room unpacking amp playing drums',\n",
       " 'alieninmusic pre excitement there s nothing like sounds almost like catch phrase']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_set = ['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from']\n",
    "content2 = [removeStopWords(stop_set,x) for x in sents_lower]\n",
    "content2[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281d13fa",
   "metadata": {},
   "source": [
    "## stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e417cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['omg just spent whole day read and do thi stupid packet',\n",
       " 'twitter war new hope now just have figur out which mac app is rebel imperi guard',\n",
       " 'donniewahlberg have seen boy you rock that logo out',\n",
       " 'today is make up day fear no spot but patchi skin',\n",
       " 'my accomplish ration and reason my mom think wa biggest mistak my life and still won t drop how frustrat',\n",
       " 'm not go be bitch so won t be choke by celia after dani write news articl about twitter',\n",
       " 'franklero haha ahhh frank you crazi kid',\n",
       " 'night night all can see more cluedo tomorrow shall just let them win so can get some peac',\n",
       " 'chaosmagick realli oh ok then just for you http twitpic com gk o',\n",
       " 'pee while hold squirmi babi is not easi task make matter wors zoe wa in stall next me poop ugh']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "def stemporter(words):\n",
    "    porter = nltk.PorterStemmer()\n",
    "    new_words = [porter.stem(w) for w in words]\n",
    "    return new_words\n",
    "    \n",
    "def stemlancaster(words):\n",
    "    porter = nltk.LancasterStemmer()\n",
    "    new_words = [porter.stem(w) for w in words]\n",
    "    return new_words    \n",
    "\n",
    "porter = [stemporter(x.split()) for x in content2]\n",
    "porter = [\" \".join(i) for i in porter]\n",
    "porter[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78463000",
   "metadata": {},
   "source": [
    "## vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e42eae6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sents_processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(analyzer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      2\u001b[0m                              preprocessor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[0;32m      3\u001b[0m                              stop_words \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      4\u001b[0m                              max_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6000\u001b[39m, ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m data_features \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43msents_processed\u001b[49m)\n\u001b[0;32m      6\u001b[0m tfidf_transformer \u001b[38;5;241m=\u001b[39m TfidfTransformer()\n\u001b[0;32m      7\u001b[0m data_features_tfidf \u001b[38;5;241m=\u001b[39m tfidf_transformer\u001b[38;5;241m.\u001b[39mfit_transform(data_features)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sents_processed' is not defined"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                             preprocessor = None, \n",
    "                             stop_words =  'english', \n",
    "                             max_features = 6000, ngram_range=(1,5))\n",
    "data_features = vectorizer.fit_transform(sents_processed)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "data_features_tfidf = tfidf_transformer.fit_transform(data_features)\n",
    "data_mat = data_features_tfidf.toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
