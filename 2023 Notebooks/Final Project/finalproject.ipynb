{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd3009a",
   "metadata": {},
   "source": [
    "# Data Science Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a0f2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tdepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "# !pip install keras\n",
    "# !pip install tensorflow\n",
    "from matplotlib import pyplot\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from dateutil import parser\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e15677",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/kazanova/sentiment140\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/06/twitter-sentiment-analysis-a-nlp-use-case-for-beginners/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bbcdb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file.\n",
      "Fixed sentiment values.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment      int32\n",
       "id           float64\n",
       "date          object\n",
       "query         object\n",
       "user          object\n",
       "tweet         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('twitterdata.csv', encoding = \"ISO-8859-1\")\n",
    "print('Loaded file.')\n",
    "df = pd.DataFrame(np.vstack([df.columns, df])) # Moves column names into row 1\n",
    "df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'tweet'] # Renames columns\n",
    "df.replace({'sentiment': {4: 1}}, inplace=True) # Replaces all '4's with '1's in column 'sentiment'\n",
    "df.replace({'sentiment': {0: -1}}, inplace=True) # Replaces all '0's with '-1's in column 'sentiment'\n",
    "data_types_dict = {\n",
    "    'sentiment': int,\n",
    "    'id': float\n",
    "}\n",
    "print('Fixed sentiment values.')\n",
    "# df['unix_time'] = pd.to_datetime(df['date'], format='%a %b %d %H:%M:%S %Z %Y', errors='coerce').astype(int) / 10**9\n",
    "# df.dropna(subset=['unix_time'], inplace=True)\n",
    "\n",
    "# df['unix_time'] = df['date'].apply(lambda x: parser.parse(x, tzinfos={\"PDT\": -7*3600}).timestamp())\n",
    "# print('Added unix column.')\n",
    "\n",
    "df = df.astype(data_types_dict)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b1cf51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@blcarol007 WAIT!  You forgot to send them up ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>http://tinyurl.com/ry9wap Good day! I like tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>@firemanfrig623 Enjoy her! They grow up too fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>Power cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>I just bricked my iPhone. Wheee!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                              tweet\n",
       "0          1  @blcarol007 WAIT!  You forgot to send them up ...\n",
       "1          1  http://tinyurl.com/ry9wap Good day! I like tra...\n",
       "2          1  @firemanfrig623 Enjoy her! They grow up too fa...\n",
       "3         -1                                         Power cut \n",
       "4         -1                 I just bricked my iPhone. Wheee!  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv('twitterdata_unix.csv', encoding = \"ISO-8859-1\")\n",
    "# df = df[['sentiment', 'tweet', 'unix_time']]\n",
    "df = df.sample(frac=1).reset_index(drop=True) # shuffles rows so its not all 1 and then -1\n",
    "df = df[['sentiment', 'tweet']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "397db2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                                                               800000\n",
      "-1                                                              799999\n",
      "isPlayer Has Died! Sorry                                           210\n",
      "good morning                                                       118\n",
      "headache                                                           115\n",
      "                                                                 ...  \n",
      "tired ..  what a nite.! oO hun? was great (:.. luv u!!xxx            1\n",
      "i love my nephew ...  have started chapter 19 of twilight =]         1\n",
      "Fuck poles                                                           1\n",
      "ICONic3 ...... why are you NOT following me ?                        1\n",
      "@crdbl I'M NOT COCKY.                                                1\n",
      "Length: 1581469, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "s = df.stack().value_counts()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80328059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@blcarol007 WAIT!  You forgot to send them up to MN first!  Silly  #YATrivia',\n",
       " 'http://tinyurl.com/ry9wap Good day! I like traveling.  Please tell me more about yourself, I am interested... I can send you my pics,  ...',\n",
       " '@firemanfrig623 Enjoy her! They grow up too fast  Love you!',\n",
       " 'Power cut',\n",
       " 'I just bricked my iPhone. Wheee!',\n",
       " 'Getting mixed signals from him....idk what to do....',\n",
       " '@prettyyinpink my rink  i dont know how im going to afford ice time now the prices are insane',\n",
       " \"hi rey!  hope you're good.\",\n",
       " '@peter_avery LMAO! Is that Dawson? Aw! No more Beaker huh?  LMAO',\n",
       " 'Winston went potty outside all by himself! such a proud momma! Now off to get my tooth drilled by my uncle...',\n",
       " '@emilysiren @michellesmiles I know! He is growing up!',\n",
       " 'hey twitter!! i forgot about you sorry',\n",
       " \"So back from the spa/sister trip. It was awesome. i'm so mellow. Even Monday doesn't scare me.\",\n",
       " \"finished weekend errands so it's time to take a long ride on the motorcycle\",\n",
       " \"I need exposure, and a few sales wouldn't hurt either\",\n",
       " \"@ayudevina a little sad   but fine! i'm watching the hills hahaha and you?\",\n",
       " '@oceanUP Ugh gross',\n",
       " '@missmaria we need jobs',\n",
       " \"@Mgpotter You're cute\",\n",
       " 'robins are back - but no sign of the blue tits for about 4 days now']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(10):\n",
    "#     print(df['tweet'][i])\n",
    "content = df['tweet'][:1000].values.tolist()\n",
    "content = [x.strip() for x in content] # Deletes white space before and after\n",
    "content[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a24891b",
   "metadata": {},
   "source": [
    "## stop words #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8c0a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blcarol wait forgot send mn first silly yatrivia',\n",
       " 'http tinyurl com ry wap good day like traveling please tell interested send pics',\n",
       " 'firemanfrig enjoy grow fast love',\n",
       " 'power cut',\n",
       " 'bricked iphone wheee',\n",
       " 'getting mixed signals idk',\n",
       " 'prettyyinpink rink dont know im going afford ice time prices insane',\n",
       " 'hi rey hope good',\n",
       " 'peter avery lmao dawson aw beaker huh lmao',\n",
       " 'winston went potty outside proud momma get tooth drilled uncle',\n",
       " 'emilysiren michellesmiles know growing',\n",
       " 'hey twitter forgot sorry',\n",
       " 'back spa sister trip awesome mellow even monday scare',\n",
       " 'finished weekend errands time take long ride motorcycle',\n",
       " 'need exposure sales hurt either',\n",
       " 'ayudevina little sad fine watching hills hahaha',\n",
       " 'oceanup ugh gross',\n",
       " 'missmaria need jobs',\n",
       " 'mgpotter cute',\n",
       " 'robins back sign blue tits days']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def full_remove(x, removal_list):\n",
    "    for w in removal_list:\n",
    "        x = x.replace(w, ' ')\n",
    "    return x\n",
    "\n",
    "## Remove digits\n",
    "digits = [str(x) for x in range(10)]\n",
    "remove_digits = [full_remove(x, digits) for x in content]\n",
    "\n",
    "## Remove punctuation\n",
    "remove_punc = [full_remove(x, list(string.punctuation)) for x in remove_digits]\n",
    "\n",
    "## Make everything lower-case and remove any white space\n",
    "sents_lower = [x.lower() for x in remove_punc]\n",
    "sents_lower = [x.strip() for x in sents_lower]\n",
    "\n",
    "## Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words(\"English\")\n",
    "def removeStopWords(stopWords, txt):\n",
    "    newtxt = ' '.join([word for word in txt.split() if word not in stopWords])\n",
    "    return newtxt\n",
    "content1 = [removeStopWords(stops,x) for x in sents_lower]\n",
    "content1[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d82a00",
   "metadata": {},
   "source": [
    "## stops we defined instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e1be13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blcarol wait you forgot send them up mn first silly yatrivia',\n",
       " 'http tinyurl com ry wap good day like traveling please tell me more about yourself am interested can send you my pics',\n",
       " 'firemanfrig enjoy her grow up too fast love you',\n",
       " 'power cut',\n",
       " 'just bricked my iphone wheee',\n",
       " 'getting mixed signals him idk what do',\n",
       " 'prettyyinpink my rink dont know how im going afford ice time now prices are insane',\n",
       " 'hi rey hope you re good',\n",
       " 'peter avery lmao is that dawson aw no more beaker huh lmao',\n",
       " 'winston went potty outside all by himself such proud momma now off get my tooth drilled by my uncle',\n",
       " 'emilysiren michellesmiles know is growing up',\n",
       " 'hey twitter forgot about you sorry',\n",
       " 'so back spa sister trip was awesome m so mellow even monday doesn t scare me',\n",
       " 'finished weekend errands so s time take long ride on motorcycle',\n",
       " 'need exposure and few sales wouldn t hurt either',\n",
       " 'ayudevina little sad but fine m watching hills hahaha and you',\n",
       " 'oceanup ugh gross',\n",
       " 'missmaria we need jobs',\n",
       " 'mgpotter you re cute',\n",
       " 'robins are back but no sign blue tits for about days now']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_set = ['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from']\n",
    "content2 = [removeStopWords(stop_set,x) for x in sents_lower]\n",
    "content2[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626aa9ce",
   "metadata": {},
   "source": [
    "## stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f973e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blcarol wait you forgot send them up mn first silli yatrivia',\n",
       " 'http tinyurl com ry wap good day like travel pleas tell me more about yourself am interest can send you my pic',\n",
       " 'firemanfrig enjoy her grow up too fast love you',\n",
       " 'power cut',\n",
       " 'just brick my iphon wheee',\n",
       " 'get mix signal him idk what do',\n",
       " 'prettyyinpink my rink dont know how im go afford ice time now price are insan',\n",
       " 'hi rey hope you re good',\n",
       " 'peter averi lmao is that dawson aw no more beaker huh lmao',\n",
       " 'winston went potti outsid all by himself such proud momma now off get my tooth drill by my uncl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "def stemporter(words):\n",
    "    porter = nltk.PorterStemmer()\n",
    "    new_words = [porter.stem(w) for w in words]\n",
    "    return new_words\n",
    "    \n",
    "def stemlancaster(words):\n",
    "    porter = nltk.LancasterStemmer()\n",
    "    new_words = [porter.stem(w) for w in words]\n",
    "    return new_words    \n",
    "\n",
    "porter = [stemporter(x.split()) for x in content2]\n",
    "porter = [\" \".join(i) for i in porter]\n",
    "porter[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ff3637",
   "metadata": {},
   "source": [
    "## vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beea9cb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sents_processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(analyzer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      2\u001b[0m                              preprocessor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[0;32m      3\u001b[0m                              stop_words \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      4\u001b[0m                              max_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6000\u001b[39m, ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m data_features \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43msents_processed\u001b[49m)\n\u001b[0;32m      6\u001b[0m tfidf_transformer \u001b[38;5;241m=\u001b[39m TfidfTransformer()\n\u001b[0;32m      7\u001b[0m data_features_tfidf \u001b[38;5;241m=\u001b[39m tfidf_transformer\u001b[38;5;241m.\u001b[39mfit_transform(data_features)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sents_processed' is not defined"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                             preprocessor = None, \n",
    "                             stop_words =  'english', \n",
    "                             max_features = 6000, ngram_range=(1,5))\n",
    "data_features = vectorizer.fit_transform(sents_processed)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "data_features_tfidf = tfidf_transformer.fit_transform(data_features)\n",
    "data_mat = data_features_tfidf.toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
