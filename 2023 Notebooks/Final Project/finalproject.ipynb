{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd3009a",
   "metadata": {},
   "source": [
    "# Data Science Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a0f2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tdepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install --user --upgrade nltk\n",
    "# !pip install --user --upgrade keras\n",
    "# !pip install --user --upgrade tensorflow\n",
    "# !pip install --user --upgrade textblob\n",
    "from matplotlib import pyplot\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from dateutil import parser\n",
    "import string\n",
    "import keras\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e15677",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/kazanova/sentiment140\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/06/twitter-sentiment-analysis-a-nlp-use-case-for-beginners/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97df18a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file.\n",
      "Fixed sentiment values.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment      int32\n",
       "id           float64\n",
       "date          object\n",
       "query         object\n",
       "user          object\n",
       "tweet         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('twitterdata.csv', encoding = \"ISO-8859-1\")\n",
    "print('Loaded file.')\n",
    "df = pd.DataFrame(np.vstack([df.columns, df])) # Moves column names into row 1\n",
    "df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'tweet'] # Renames columns\n",
    "df.replace({'sentiment': {4: 1}}, inplace=True) # Replaces all '4's with '1's in column 'sentiment'\n",
    "df.replace({'sentiment': {0: -1}}, inplace=True) # Replaces all '0's with '-1's in column 'sentiment'\n",
    "data_types_dict = {\n",
    "    'sentiment': int,\n",
    "    'id': float\n",
    "}\n",
    "print('Fixed sentiment values.')\n",
    "# df['unix_time'] = pd.to_datetime(df['date'], format='%a %b %d %H:%M:%S %Z %Y', errors='coerce').astype(int) / 10**9\n",
    "# df.dropna(subset=['unix_time'], inplace=True)\n",
    "\n",
    "# df['unix_time'] = df['date'].apply(lambda x: parser.parse(x, tzinfos={\"PDT\": -7*3600}).timestamp())\n",
    "# print('Added unix column.')\n",
    "\n",
    "df = df.astype(data_types_dict)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b1cf51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Watching the laker game at a nigga's house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@jaanus83 i was listening to gervais podcasts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>@leonkay Big Brother's turned you into a doggy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5 points with Chris &amp;amp; Matt. Getting some A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Tonight is the season finale! - don't miss a v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                              tweet\n",
       "0          1        Watching the laker game at a nigga's house \n",
       "1          1  @jaanus83 i was listening to gervais podcasts ...\n",
       "2          1  @leonkay Big Brother's turned you into a doggy...\n",
       "3          1  5 points with Chris &amp; Matt. Getting some A...\n",
       "4          1  Tonight is the season finale! - don't miss a v..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv('twitterdata_unix.csv', encoding = \"ISO-8859-1\")\n",
    "# df = df[['sentiment', 'tweet', 'unix_time']]\n",
    "df = df.sample(frac=1).reset_index(drop=True) # shuffles rows so its not all 1 and then -1\n",
    "df = df[['sentiment', 'tweet']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "397db2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                                                                                                                                800000\n",
      "-1                                                                                                                               799999\n",
      "isPlayer Has Died! Sorry                                                                                                            210\n",
      "good morning                                                                                                                        118\n",
      "headache                                                                                                                            115\n",
      "                                                                                                                                  ...  \n",
      "@leomakkinje hey I'm on vacation, I'm supposed to do things I like  like sitting next to a pc user with my shiny powerbook ;)         1\n",
      "@MeghnaK Yes, he does   http://bit.ly/dNmPx And i just deleted his last song off my library.                                          1\n",
      "omigod.. feel so cheated..                                                                                                            1\n",
      "I am going to fail                                                                                                                    1\n",
      "We all have bills to pay right? What if we were about more than payday? New post at http://tinyurl.com/ko3dlh  check it out           1\n",
      "Length: 1581469, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "s = df.stack().value_counts()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80328059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Watching the laker game at a nigga's house\",\n",
       " '@jaanus83 i was listening to gervais podcasts not 10 minutes ago',\n",
       " \"@leonkay Big Brother's turned you into a doggy woof woof  x\",\n",
       " '5 points with Chris &amp; Matt. Getting some Als and then going to listen to a band',\n",
       " \"Tonight is the season finale! - don't miss a very BIG CALVIN &amp; GRANT Surprise! ABC Family 8pm @abcfam  (via @GregoryMichael)\",\n",
       " \"Want to go play in the rain but i've only just washed my hair  boooooooo\",\n",
       " \"...fresh out the jaccuzi...its late but i'm starving..bout to cook..steak, mashed potatoes &amp; corn...mmmmm\",\n",
       " 'http://twitpic.com/6bq78 - look my McFly Shoes',\n",
       " '@vaxen_var Have both The Natural Way of Farming and One Straw Revolution (first editions). My dog chewed on one',\n",
       " 'I am a clump of fowl  sunstroke. Death.',\n",
       " '@dannieboyTV:  I know. I can maybe fix that though...',\n",
       " '@SheilaBrummer Glad to see you twittering again!!!',\n",
       " \"I am so excited about The Script on Friday now  2 doses of tv featuring them and I'm excited\",\n",
       " \"@Hybrid911 can't o get rid  used to be fine... only started doing this yesterday\",\n",
       " \"I'm cold\",\n",
       " 'Saved 30 dollars at the grocery store with coupons!',\n",
       " 'Enjoyed my dominos  I rented Bolt and The Spirit. Gran Torino was out. Also rented terminator salvation for an easy 1k',\n",
       " 'Grrr that meant to be a  not a :*',\n",
       " 'Left Digsby running overnight. Apparently I went un-idle several times; got messages from people who thought I was up at 4 a.m.',\n",
       " 'trying to figgure out this twitter lark']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(10):\n",
    "#     print(df['tweet'][i])\n",
    "content = df['tweet'][:5000].values.tolist()\n",
    "labels = df['sentiment'][:5000].values.tolist()\n",
    "content = [x.strip() for x in content] # Deletes white space before and after\n",
    "content[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c90fd842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1,  1, -1,  1,  1, -1, -1, -1,  1,  1, -1, -1,  1,  1,\n",
       "       -1,  1, -1], dtype=int8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(labels, dtype='int8')\n",
    "y[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae2b56f",
   "metadata": {},
   "source": [
    "## stop words #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8c0a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['watching laker game nigga house',\n",
       " 'jaanus listening gervais podcasts minutes ago',\n",
       " 'leonkay big brother turned doggy woof woof x',\n",
       " 'points chris amp matt getting als going listen band',\n",
       " 'tonight season finale miss big calvin amp grant surprise abc family pm abcfam via gregorymichael',\n",
       " 'want go play rain washed hair boooooooo',\n",
       " 'fresh jaccuzi late starving bout cook steak mashed potatoes amp corn mmmmm',\n",
       " 'http twitpic com bq look mcfly shoes',\n",
       " 'vaxen var natural way farming one straw revolution first editions dog chewed one',\n",
       " 'clump fowl sunstroke death',\n",
       " 'dannieboytv know maybe fix though',\n",
       " 'sheilabrummer glad see twittering',\n",
       " 'excited script friday doses tv featuring excited',\n",
       " 'hybrid get rid used fine started yesterday',\n",
       " 'cold',\n",
       " 'saved dollars grocery store coupons',\n",
       " 'enjoyed dominos rented bolt spirit gran torino also rented terminator salvation easy k',\n",
       " 'grrr meant',\n",
       " 'left digsby running overnight apparently went un idle several times got messages people thought',\n",
       " 'trying figgure twitter lark']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def full_remove(x, removal_list):\n",
    "    for w in removal_list:\n",
    "        x = x.replace(w, ' ')\n",
    "    return x\n",
    "\n",
    "## Remove digits\n",
    "digits = [str(x) for x in range(10)]\n",
    "remove_digits = [full_remove(x, digits) for x in content]\n",
    "\n",
    "## Remove punctuation\n",
    "remove_punc = [full_remove(x, list(string.punctuation)) for x in remove_digits]\n",
    "\n",
    "## Make everything lower-case and remove any white space\n",
    "sents_lower = [x.lower() for x in remove_punc]\n",
    "sents_lower = [x.strip() for x in sents_lower]\n",
    "\n",
    "## Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words(\"English\")\n",
    "def removeStopWords(stopWords, txt):\n",
    "    newtxt = ' '.join([word for word in txt.split() if word not in stopWords])\n",
    "    return newtxt\n",
    "content1 = [removeStopWords(stops,x) for x in sents_lower]\n",
    "content1[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76465a0",
   "metadata": {},
   "source": [
    "## stops we defined instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de5a9887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['watching laker game at nigga s house',\n",
       " 'jaanus was listening gervais podcasts not minutes ago',\n",
       " 'leonkay big brother s turned you into doggy woof woof x',\n",
       " 'points with chris amp matt getting some als and then going listen band',\n",
       " 'tonight is season finale don t miss very big calvin amp grant surprise abc family pm abcfam via gregorymichael',\n",
       " 'want go play in rain but ve only just washed my hair boooooooo',\n",
       " 'fresh out jaccuzi its late but m starving bout cook steak mashed potatoes amp corn mmmmm',\n",
       " 'http twitpic com bq look my mcfly shoes',\n",
       " 'vaxen var have both natural way farming and one straw revolution first editions my dog chewed on one',\n",
       " 'am clump fowl sunstroke death',\n",
       " 'dannieboytv know can maybe fix that though',\n",
       " 'sheilabrummer glad see you twittering again',\n",
       " 'am so excited about script on friday now doses tv featuring them and m excited',\n",
       " 'hybrid can t o get rid used be fine only started doing this yesterday',\n",
       " 'm cold',\n",
       " 'saved dollars at grocery store with coupons',\n",
       " 'enjoyed my dominos rented bolt and spirit gran torino was out also rented terminator salvation for easy k',\n",
       " 'grrr that meant be not',\n",
       " 'left digsby running overnight apparently went un idle several times got messages people who thought was up at m',\n",
       " 'trying figgure out this twitter lark']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_set = ['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from']\n",
    "content2 = [removeStopWords(stop_set,x) for x in sents_lower]\n",
    "content2[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281d13fa",
   "metadata": {},
   "source": [
    "## stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e417cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['watch laker game at nigga s hous',\n",
       " 'jaanu wa listen gervai podcast not minut ago',\n",
       " 'leonkay big brother s turn you into doggi woof woof x',\n",
       " 'point with chri amp matt get some al and then go listen band',\n",
       " 'tonight is season final don t miss veri big calvin amp grant surpris abc famili pm abcfam via gregorymichael',\n",
       " 'want go play in rain but ve onli just wash my hair boooooooo',\n",
       " 'fresh out jaccuzi it late but m starv bout cook steak mash potato amp corn mmmmm',\n",
       " 'http twitpic com bq look my mcfli shoe',\n",
       " 'vaxen var have both natur way farm and one straw revolut first edit my dog chew on one',\n",
       " 'am clump fowl sunstrok death']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "def stemporter(words):\n",
    "    porter = nltk.PorterStemmer()\n",
    "    new_words = [porter.stem(w) for w in words]\n",
    "    return new_words\n",
    "    \n",
    "def stemlancaster(words):\n",
    "    porter = nltk.LancasterStemmer()\n",
    "    new_words = [porter.stem(w) for w in words]\n",
    "    return new_words    \n",
    "\n",
    "porter = [stemporter(x.split()) for x in content2]\n",
    "porter = [\" \".join(i) for i in porter]\n",
    "porter[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78463000",
   "metadata": {},
   "source": [
    "## vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e42eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                             preprocessor = None, \n",
    "                             stop_words =  'english', \n",
    "                             max_features = 6000, ngram_range=(1,5))\n",
    "data_features = vectorizer.fit_transform(content2)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "data_features_tfidf = tfidf_transformer.fit_transform(data_features)\n",
    "data_mat = data_features_tfidf.toarray()\n",
    "\n",
    "np.random.seed(0)\n",
    "test_index = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "train_index = list(set(range(len(labels))) - set(test_index))\n",
    "train_data = data_mat[train_index,]\n",
    "train_labels = y[train_index]\n",
    "test_data = data_mat[test_index,]\n",
    "test_labels = y[test_index]\n",
    "\n",
    "#Create polarity function and subjectivity function\n",
    "pol = lambda x: TextBlob(x).sentiment.polarity\n",
    "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "pol_list = [pol(x) for x in content2]\n",
    "sub_list = [sub(x) for x in content2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01647972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watching laker game at nigga s house \t -0.4 0.4\n",
      "jaanus was listening gervais podcasts not minutes ago \t 0.0 0.0\n",
      "leonkay big brother s turned you into doggy woof woof x \t 0.0 0.1\n",
      "points with chris amp matt getting some als and then going listen band \t 0.0 0.0\n",
      "tonight is season finale don t miss very big calvin amp grant surprise abc family pm abcfam via gregorymichael \t 0.0 0.13\n",
      "want go play in rain but ve only just washed my hair boooooooo \t 0.0 1.0\n",
      "fresh out jaccuzi its late but m starving bout cook steak mashed potatoes amp corn mmmmm \t 0.0 0.55\n",
      "http twitpic com bq look my mcfly shoes \t 0.0 0.0\n",
      "vaxen var have both natural way farming and one straw revolution first editions my dog chewed on one \t 0.175 0.3666666666666667\n",
      "am clump fowl sunstroke death \t 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(content2[i], '\\t', pol_list[i], sub_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b603ea95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error:  0.04688888888888889\n",
      "Test error:  0.348\n"
     ]
    }
   ],
   "source": [
    "## Fit logistic classifier on training data\n",
    "clf = SGDClassifier(loss=\"log\", penalty=\"none\")\n",
    "clf.fit(train_data, train_labels)\n",
    "## Pull out the parameters (w,b) of the logistic regression model\n",
    "w = clf.coef_[0,:]\n",
    "b = clf.intercept_\n",
    "## Get predictions on training and test data\n",
    "preds_train = clf.predict(train_data)\n",
    "preds_test = clf.predict(test_data)\n",
    "## Compute errors\n",
    "errs_train = np.sum((preds_train > 0.0) != (train_labels > 0.0))\n",
    "errs_test = np.sum((preds_test > 0.0) != (test_labels > 0.0))\n",
    "print(\"Training error: \", float(errs_train)/len(train_labels))\n",
    "print(\"Test error: \", float(errs_test)/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecfbf231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly negative words: \n",
      "['sad', 'headache', 'poor', 'scared', 'unfortunately', 'happened', 'ughhhh', 'missing', 'sick', 'understand', 'infection', 'sadly', 'wish', 'upset', 'faster', 'stopped', 'lebron', 'mail', 'didnt', 'weather good', 'monday', 'sucks', 'prayers', 'marco', 'work', 'complete', 'lost', 'shame', 'xxx', 'revision', 'realized', 'bad', 'answer', 'bathroom', 'hahahaha', 'henry', 'rain', 'xox', 'downtown', 'cancelled', 'babysitting', 'ahhhhhh', 'procrastinator', 'website', 'stage', 'oh gosh', 'taken', 'english', 'hun', 'haven']\n",
      "Highly positive words: \n",
      "['mommy', 'called', 'dating', 'did miss', 'meal', 'expect', 'loved', 'purple', 'welcome', 'concert', 'entirely', 'time tweet', 'drink', 'want hear', 'mis', 'hello', 'cookies', 'food', 'dannygokey', 'florida', 'beer', 'excited', 'anytime', 'hillsong', 'ass', 'lot thanks', 'reasons', 'fridge', 'official', 'process', 'fantastic', 'thanks', 'cindy', 'nice', 'ah', 'cute', 'like ll', 'fabulous', 'rocks', 'listening', 'sunshine', 'say hello', 'mind', 'sweet', 'awesome', 'patio', 'sweetheart', 'hi']\n"
     ]
    }
   ],
   "source": [
    "## Convert vocabulary into a list:\n",
    "vocab = np.array([z[0] for z in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])])\n",
    "## Get indices of sorting w\n",
    "inds = np.argsort(w)\n",
    "## Words with large negative values\n",
    "neg_inds = inds[0:50]\n",
    "print(\"Highly negative words: \")\n",
    "# MB: fixed bug here\n",
    "print([x for x in list(vocab[neg_inds])])\n",
    "## Words with large positive values\n",
    "pos_inds = inds[-49:-1]\n",
    "print(\"Highly positive words: \")\n",
    "print([x for x in list(vocab[pos_inds])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf5ac9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1]\n",
      "[1]\n",
      "[1]\n",
      "[-1]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(vectorizer.transform([\"It's a sad movie but very good\"])))\n",
    "print(clf.predict(vectorizer.transform([\"Waste of my time\"])))\n",
    "print(clf.predict(vectorizer.transform([\"It is not what like\"])))\n",
    "print(clf.predict(vectorizer.transform([\"It is not what I m looking for\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e85cca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error:  0.308\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "svm_clf = SGDClassifier(loss=\"hinge\", penalty='l2')\n",
    "svm_clf.fit(train_data, train_labels)\n",
    "svm_preds_test = svm_clf.predict(test_data)\n",
    "svm_errs_test = np.sum((svm_preds_test > 0.0) != (test_labels > 0.0))\n",
    "print(\"Test error: \", float(svm_errs_test)/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f938b170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1]\n",
      "[-1]\n",
      "[1]\n",
      "[-1]\n"
     ]
    }
   ],
   "source": [
    "print(svm_clf.predict(vectorizer.transform([\"It's a sad movie but very good\"])))\n",
    "print(svm_clf.predict(vectorizer.transform([\"Waste of my time\"])))\n",
    "print(svm_clf.predict(vectorizer.transform([\"This is not what I like\"])))\n",
    "print(svm_clf.predict(vectorizer.transform([\"It is not what I am looking for\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f9f4baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 200\n",
    "tokenizer = Tokenizer(num_words=10000,  #max no. of unique words to keep\n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', \n",
    "                      lower=True #convert to lower case\n",
    "                     )\n",
    "tokenizer.fit_on_texts(content2)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(content2)\n",
    "X = pad_sequences(X, maxlen= max_review_length)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "\n",
    "Y=pd.get_dummies(y).values\n",
    "\n",
    "np.random.seed(0)\n",
    "test_inds = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "train_inds = list(set(range(len(labels))) - set(test_inds))\n",
    "train_data = X[train_inds,]\n",
    "train_labels = Y[train_inds]\n",
    "test_data = X[test_inds,]\n",
    "test_labels = Y[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3007dc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 200)          2000000   \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 200, 200)         0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 200, 250)          451000    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               140400    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,591,602\n",
      "Trainable params: 2,591,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(250, dropout=0.2,return_sequences=True))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ccae24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 75/102 [=====================>........] - ETA: 31s - loss: 0.6556 - accuracy: 0.6100"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "batch_size = 40\n",
    "model.fit(train_data, train_labels, \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f8563",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_data, test_labels, verbose=2,\n",
    "                            batch_size=batch_size)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Validation accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6e6299",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_labels = ['Negative', 'Positive']\n",
    "new = [\"kill yourself\"]\n",
    " \n",
    "def predict_sentiment(text):\n",
    "    seq = tokenizer.texts_to_sequences(text)\n",
    "    padded = pad_sequences(seq, maxlen=max_review_length)\n",
    "    pred = model.predict(padded)\n",
    "    print(\"Probability distribution: \", pred)\n",
    "    print(f\"Is this a Positive or Negative message? '{new[0]}'\")\n",
    "    print(outcome_labels[np.argmax(pred)])\n",
    "\n",
    "predict_sentiment(new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
