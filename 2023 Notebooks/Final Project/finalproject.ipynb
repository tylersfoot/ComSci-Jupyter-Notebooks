{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd3009a",
   "metadata": {},
   "source": [
    "# Data Science Final Project\n",
    "\n",
    "\n",
    "For my final project, I chose to utilize Natural Language Processing (NLP) for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a0f2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tdepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# install packages (for use on a differect computer)\n",
    "# !pip install --user --upgrade nltk\n",
    "# !pip install --user --upgrade keras\n",
    "# !pip install --user --upgrade tensorflow\n",
    "# !pip install --user --upgrade textblob\n",
    "# !pip install --user --upgrade tqdm\n",
    "# !pip install --user --upgrade time\n",
    "\n",
    "# for progress bars\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from dateutil import parser\n",
    "import string\n",
    "import keras\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.max_colwidth = 1000\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e15677",
   "metadata": {},
   "source": [
    "Here are some of the references that I used (for downloading the data and with code help)\n",
    "\n",
    "https://www.kaggle.com/datasets/kazanova/sentiment140\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/06/twitter-sentiment-analysis-a-nlp-use-case-for-beginners/\n",
    "\n",
    "https://www.kaggle.com/datasets/columbine/imdb-dataset-sentiment-analysis-in-csv-format\n",
    "\n",
    "https://www.kaggle.com/datasets/gpreda/covid19-tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8953d40",
   "metadata": {},
   "source": [
    "## Importing Dataset\n",
    "\n",
    "Here, I import the dataset containing the pre-analyzed tweets. I fix the dataframe a little to work better with the machine learning model. I also shuffle the dataset, as it is originally sorted with all positives first and then negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97df18a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment      int32\n",
       "id           float64\n",
       "date          object\n",
       "query         object\n",
       "user          object\n",
       "tweet         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('twitterdata.csv', encoding = \"ISO-8859-1\")\n",
    "print('Loaded file.')\n",
    "df = pd.DataFrame(np.vstack([df.columns, df])) # Moves column names into row 1\n",
    "df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'tweet'] # Renames columns\n",
    "df.replace({'sentiment': {4: 1}}, inplace=True) # Replaces all '4's with '1's in column 'sentiment'\n",
    "df.replace({'sentiment': {0: -1}}, inplace=True) # Replaces all '0's with '-1's in column 'sentiment'\n",
    "data_types_dict = {\n",
    "    'sentiment': int,\n",
    "    'id': float\n",
    "}\n",
    "df = df.astype(data_types_dict) # changes value types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b1cf51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@KyeGrace transfer tax on Friday? You need a tweetup drink asap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>I'm up way to early  I could sleep so much longer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>I havent found my cell phone  its been lost since last saturday.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>What's going on in Iran is disgusting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>Oh darn-it looks like Linq to NHibernate doesn't implement the join operator ?yet?   Otherwise it's looking super kewl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  \\\n",
       "0          1   \n",
       "1         -1   \n",
       "2         -1   \n",
       "3         -1   \n",
       "4         -1   \n",
       "\n",
       "                                                                                                                    tweet  \n",
       "0                                                        @KyeGrace transfer tax on Friday? You need a tweetup drink asap   \n",
       "1                                                                       I'm up way to early  I could sleep so much longer  \n",
       "2                                                        I havent found my cell phone  its been lost since last saturday.  \n",
       "3                                                                                  What's going on in Iran is disgusting   \n",
       "4  Oh darn-it looks like Linq to NHibernate doesn't implement the join operator ?yet?   Otherwise it's looking super kewl  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True) # shuffles rows so its not all 1 and then -1\n",
    "df = df[['sentiment', 'tweet']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "397db2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                                                                                                                         800000\n",
      "-1                                                                                                                        799999\n",
      "isPlayer Has Died! Sorry                                                                                                     210\n",
      "good morning                                                                                                                 118\n",
      "headache                                                                                                                     115\n",
      "                                                                                                                           ...  \n",
      "Longing for weekend... It's only Tuesday today..                                                                               1\n",
      "@tweeetybabi Get 100 followers a day using www.tweeteradder.com Once you add everyone you are on the train or pay vip          1\n",
      "argh lots of clouds! Is this the end of our mini heatwave?  #Newcastle                                                         1\n",
      "@beauty411 nice way to spend the day                                                                                           1\n",
      "@AznDiva I wish they go and play golf everyday   I had a great time on Friday.                                                 1\n",
      "Length: 1581469, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "s = df.stack().value_counts()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1344ca65",
   "metadata": {},
   "source": [
    "Here, since the dataset is very large, I can limit the amount of rows used to run faster while testing as `cutoff`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80328059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@KyeGrace transfer tax on Friday? You need a tweetup drink asap',\n",
       " \"I'm up way to early  I could sleep so much longer\",\n",
       " 'I havent found my cell phone  its been lost since last saturday.',\n",
       " \"What's going on in Iran is disgusting\",\n",
       " \"Oh darn-it looks like Linq to NHibernate doesn't implement the join operator ?yet?   Otherwise it's looking super kewl\",\n",
       " \"@shelbyparkin ... &amp; careful you don't get ice cream in your homework my spelling challenged daughter\",\n",
       " \"gonna catch the movie Angels &amp; Demons tonight, can't wait!\",\n",
       " \"fell asleep on the sofa now I'm gonna beup all night\",\n",
       " \"even if you're going to show grace, getting their information is always a good idea, just in case... oh well.... a little bummed  g'nite\",\n",
       " \"@melgal7 omg... hope that this is not true... it'll suck if it is!!!\",\n",
       " \"@hugofirth Yo dude, let me know if you want any articles/reviews done on some web stuff and I'm sure I could write something up for you!\",\n",
       " '@bytera  You know I have these instruments collecting dust   Used to be so much part of my life....should play more again',\n",
       " 'How is it that I have a cold in the summer!! Poo.',\n",
       " '@adam_keun absolutely, stay tuned',\n",
       " \"my ipod!  water got on it and now i think it's dead!!! Oh please let my ipod work!!\",\n",
       " '@chr1st0pher bidding on some new tickets',\n",
       " '@mcdayton  Miranda will treat it well.',\n",
       " '@LauraKelly28 Regent Street has been (and still is) incredibly sunny and crowded.',\n",
       " '@CC_Cassin Cheers. He had flopped flush but 32 is a fold. I had AK and flopped 2 pair',\n",
       " 'hubby has left for maine... so sad &amp; lonely']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutoff = 50\n",
    "content = df['tweet'][:cutoff].values.tolist()\n",
    "labels = df['sentiment'][:cutoff].values.tolist()\n",
    "content = [x.strip() for x in content] # Deletes white space before and after\n",
    "content[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63626de3",
   "metadata": {},
   "source": [
    "We create `y` as a numpy array of all labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4308341a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1, -1, -1, -1,  1,  1, -1, -1, -1,  1, -1, -1,  1, -1,  1, -1,\n",
       "        1, -1, -1], dtype=int8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(labels, dtype='int8')\n",
    "y[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae2b56f",
   "metadata": {},
   "source": [
    "## Stop Words #1\n",
    "\n",
    "We create a dataframe `content1` that is processed through nltk's list of stop words. Basically, we go through each tweet and remove each of the 'stop words' that do not add any meaning to make it easier for the model to train on. It also removes other unneccesary information, such as numbers and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d8c0a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kyegrace transfer tax friday need tweetup drink asap',\n",
       " 'way early could sleep much longer',\n",
       " 'havent found cell phone lost since last saturday',\n",
       " 'going iran disgusting',\n",
       " 'oh darn looks like linq nhibernate implement join operator yet otherwise looking super kewl',\n",
       " 'shelbyparkin amp careful get ice cream homework spelling challenged daughter',\n",
       " 'gonna catch movie angels amp demons tonight wait',\n",
       " 'fell asleep sofa gonna beup night',\n",
       " 'even going show grace getting information always good idea case oh well little bummed g nite',\n",
       " 'melgal omg hope true suck',\n",
       " 'hugofirth yo dude let know want articles reviews done web stuff sure could write something',\n",
       " 'bytera know instruments collecting dust used much part life play',\n",
       " 'cold summer poo',\n",
       " 'adam keun absolutely stay tuned',\n",
       " 'ipod water got think dead oh please let ipod work',\n",
       " 'chr st pher bidding new tickets',\n",
       " 'mcdayton miranda treat well',\n",
       " 'laurakelly regent street still incredibly sunny crowded',\n",
       " 'cc cassin cheers flopped flush fold ak flopped pair',\n",
       " 'hubby left maine sad amp lonely']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def full_remove(x, removal_list):\n",
    "    # function for removing the stop words\n",
    "    for w in removal_list:\n",
    "        x = x.replace(w, ' ')\n",
    "    return x\n",
    "\n",
    "# remove all digits\n",
    "digits = [str(x) for x in range(10)]\n",
    "remove_digits = [full_remove(x, digits) for x in content]\n",
    "\n",
    "# remove all punctuation\n",
    "remove_punc = [full_remove(x, list(string.punctuation)) for x in remove_digits]\n",
    "\n",
    "# make everything lower-case and remove any white space\n",
    "sents_lower = [x.lower() for x in remove_punc]\n",
    "sents_lower = [x.strip() for x in sents_lower]\n",
    "\n",
    "# remove stop words\n",
    "stops = stopwords.words(\"English\")\n",
    "def removeStopWords(stopWords, txt):\n",
    "    newtxt = ' '.join([word for word in txt.split() if word not in stopWords])\n",
    "    return newtxt\n",
    "content1 = [removeStopWords(stops,x) for x in sents_lower]\n",
    "content1[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78463000",
   "metadata": {},
   "source": [
    "## Vectorizing\n",
    "\n",
    "explain here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e42eae6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m data_mat \u001b[38;5;241m=\u001b[39m data_features_tfidf\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     10\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m test_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m, np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice((np\u001b[38;5;241m.\u001b[39mwhere(y\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m))[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m250\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m     12\u001b[0m train_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(labels))) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(test_index))\n\u001b[0;32m     13\u001b[0m train_data \u001b[38;5;241m=\u001b[39m data_mat[train_index,]\n",
      "File \u001b[1;32mmtrand.pyx:965\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                             preprocessor = None, \n",
    "                             stop_words =  'english', \n",
    "                             max_features = 6000, ngram_range=(1,5))\n",
    "data_features = vectorizer.fit_transform(content1)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "data_features_tfidf = tfidf_transformer.fit_transform(data_features)\n",
    "data_mat = data_features_tfidf.toarray()\n",
    "\n",
    "np.random.seed(0)\n",
    "test_index = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "train_index = list(set(range(len(labels))) - set(test_index))\n",
    "train_data = data_mat[train_index,]\n",
    "train_labels = y[train_index]\n",
    "test_data = data_mat[test_index,]\n",
    "test_labels = y[test_index]\n",
    "\n",
    "# create polarity function and subjectivity function\n",
    "pol = lambda x: TextBlob(x).sentiment.polarity\n",
    "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "pol_list = [pol(x) for x in content1]\n",
    "sub_list = [sub(x) for x in content1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cfd8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(content2[i], '\\t', pol_list[i], sub_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit logistic classifier on training data\n",
    "clf = SGDClassifier(loss=\"log\", penalty=\"none\")\n",
    "clf.fit(train_data, train_labels)\n",
    "## Pull out the parameters (w,b) of the logistic regression model\n",
    "w = clf.coef_[0,:]\n",
    "b = clf.intercept_\n",
    "## Get predictions on training and test data\n",
    "preds_train = clf.predict(train_data)\n",
    "preds_test = clf.predict(test_data)\n",
    "## Compute errors\n",
    "errs_train = np.sum((preds_train > 0.0) != (train_labels > 0.0))\n",
    "errs_test = np.sum((preds_test > 0.0) != (test_labels > 0.0))\n",
    "print(\"Training error: \", float(errs_train)/len(train_labels))\n",
    "print(\"Test error: \", float(errs_test)/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab07ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert vocabulary into a list:\n",
    "vocab = np.array([z[0] for z in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])])\n",
    "## Get indices of sorting w\n",
    "inds = np.argsort(w)\n",
    "## Words with large negative values\n",
    "neg_inds = inds[0:50]\n",
    "print(\"Highly negative words: \")\n",
    "# MB: fixed bug here\n",
    "print([x for x in list(vocab[neg_inds])])\n",
    "## Words with large positive values\n",
    "pos_inds = inds[-49:-1]\n",
    "print(\"Highly positive words: \")\n",
    "print([x for x in list(vocab[pos_inds])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badb01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.predict(vectorizer.transform([\"It's a sad movie but very good\"])))\n",
    "print(clf.predict(vectorizer.transform([\"Waste of my time\"])))\n",
    "print(clf.predict(vectorizer.transform([\"It is not what like\"])))\n",
    "print(clf.predict(vectorizer.transform([\"It is not what I m looking for\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09871006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "svm_clf = SGDClassifier(loss=\"hinge\", penalty='l2')\n",
    "svm_clf.fit(train_data, train_labels)\n",
    "svm_preds_test = svm_clf.predict(test_data)\n",
    "svm_errs_test = np.sum((svm_preds_test > 0.0) != (test_labels > 0.0))\n",
    "print(\"Test error: \", float(svm_errs_test)/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54900f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svm_clf.predict(vectorizer.transform([\"It's a sad movie but very good\"])))\n",
    "print(svm_clf.predict(vectorizer.transform([\"Waste of my time\"])))\n",
    "print(svm_clf.predict(vectorizer.transform([\"This is not what I like\"])))\n",
    "print(svm_clf.predict(vectorizer.transform([\"It is not what I am looking for\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538d272",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = 200\n",
    "tokenizer = Tokenizer(num_words=10000,  #max no. of unique words to keep\n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', \n",
    "                      lower=True #convert to lower case\n",
    "                     )\n",
    "tokenizer.fit_on_texts(content2)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(content2)\n",
    "X = pad_sequences(X, maxlen= max_review_length)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "\n",
    "Y=pd.get_dummies(y).values\n",
    "\n",
    "np.random.seed(0)\n",
    "test_inds = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "train_inds = list(set(range(len(labels))) - set(test_inds))\n",
    "train_data = X[train_inds,]\n",
    "train_labels = Y[train_inds]\n",
    "test_data = X[test_inds,]\n",
    "test_labels = Y[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586a9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(250, dropout=0.2,return_sequences=True))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a3d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 40\n",
    "model.fit(train_data, train_labels, \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff243fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_data, test_labels, verbose=2,\n",
    "                            batch_size=batch_size)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Validation accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_labels = ['Negative', 'Positive']\n",
    "new = [\"test\"]\n",
    " \n",
    "def predict_sentiment(text, pr):\n",
    "    seq = tokenizer.texts_to_sequences(text)\n",
    "    padded = pad_sequences(seq, maxlen=max_review_length)\n",
    "    if pr:\n",
    "        pred = model.predict(padded)\n",
    "        print(\"Probability distribution: \", pred)\n",
    "        print(f\"Is this a Positive or Negative message? '{text[0]}'\")\n",
    "        print(outcome_labels[np.argmax(pred)])\n",
    "    else:\n",
    "        pred = model.predict(padded, verbose=0)\n",
    "    return outcome_labels[np.argmax(pred)]\n",
    "\n",
    "predict_sentiment(new, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f0142",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_sentiment(['kys'], True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82efb81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcovid = pd.read_csv('covid19_tweets.csv')\n",
    "dfcovid = dfcovid[['text']]\n",
    "dfcovid['text'] = dfcovid['text'].str.replace(r'https?://\\S+', '', case=False)\n",
    "dfcovid = dfcovid[:5000]\n",
    "dfcovid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952a32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcovid['result'] = dfcovid.progress_apply(lambda row: predict_sentiment([row.text], False), axis=1)\n",
    "dfcovid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf984416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seaborn to create a bar graph\n",
    "sns.countplot(x='result', data=dfcovid, palette='Blues')\n",
    "\n",
    "# Add labels and show the plot\n",
    "plt.xlabel('Result')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of -1 vs 1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30150928",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
